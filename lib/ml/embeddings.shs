@define(ml/cache-path "" IgnoreRedefined: true)
@define(ml/embeddings/max-tokens 512)
@define(ml/embeddings/overlap 64)
@define(ml/embeddings/chunk-size #(512 | Sub(64)))
@define(ml/embeddings/size 384)
@define(ml/embeddings/model-url "https://develop.fragcolor.com/noinstruct-small-embedding-v0-q5_k_m.gguf")
@define(ml/embeddings/model-file "embeddings_v1.gguf")

@wire(ml/embeddings/download-deps {
  [@ml/cache-path @ml/embeddings/model-file] | String.Join = model-path
  WhenNot(FS.IsFile {
    none | Http.Get(URL: @ml/embeddings/model-url Bytes: true Timeout: 120 Retry: 2) = model-bytes
    model-path | FS.Write(model-bytes Overwrite: true)
  })
  model-path
})

@wire(ml/embeddings/load {
  Do(ml/embeddings/download-deps)
  Await({
    LLM.Model(UseMmap: false) = model ; small model, keep in RAM
    LLM.Context(Embeddings: true) = context
  })
  {
    model: model
    context: context
  }
} Pure: true)

@define(ml/embeddings/prepare {
  Once({
    Msg("Loading embeddings model...")
    Detach(ml/embeddings/load) ; this allows us to Stop/Cleanup md/load once everything is loaded
    Wait(ml/embeddings/load)
    {Take("model") = ml/embeddings/model}
    {Take("context") = ml/embeddings/context}
  })
})

@wire(ml/embed/activate {
  ; share a wire, to allocate just one context
  LLM.Embed(ml/embeddings/context 2)
})

@wire(ml/embed {
  Await({
    LLM.Tokenize(ml/embeddings/model) = tokens
    Count(tokens)
    If(IsMore(@ml/embeddings/max-tokens) {
        = num-tokens
        0 >= processed
        Sequence(chunks Type: @type([[Type::Float]]))
        Repeat({
          ; compute the end index for the current chunk
          processed | Add(@ml/embeddings/max-tokens) | Min(num-tokens) = slice-end
          slice-end | WhenNot(IsMore(processed) Return) ; early exit if we've processed all tokens here
          tokens | Slice(From: processed To: slice-end) = tokens-chunk
          ; notice we advance by chunk-size, not max-tokens, to leave an overlap
          processed | Add(@ml/embeddings/chunk-size) > processed
          
          tokens-chunk | Do(ml/embed/activate) >> chunks
        } Until: {processed | IsMoreEqual(num-tokens)})
        chunks
      } {
        tokens | Do(ml/embed/activate) = embeddings
        [embeddings]
      }
    )
  })
})

@wire(ml/cosine-similarity {
  {Take(0) | Tensor([@ml/embeddings/size]) = tensor-a}
  {Take(1) | Tensor([@ml/embeddings/size]) = tensor-b}
  tensor-a | Tensor.Mul(tensor-b) | Tensor.Sum | Tensor.ToFloat = sum-ij
  tensor-a | Tensor.Mul(tensor-a) | Tensor.Sum | Tensor.ToFloat = sum-i2
  tensor-b | Tensor.Mul(tensor-b) | Tensor.Sum | Tensor.ToFloat = sum-j2
  sum-i2 | Mul(sum-j2) | Sqrt = sqrt-ij
  sum-ij | Div(sqrt-ij)
} Pure: true)
